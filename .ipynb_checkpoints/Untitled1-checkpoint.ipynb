{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapcontext( synset, sentence ):\n",
    "    \n",
    "    gloss = set(nltk.word_tokenize(synset.gloss()))\n",
    "    \n",
    "    for i in synset.examples():\n",
    "        gloss=gloss.union(set(nltk.word_tokenize(i)))\n",
    "    \n",
    "    if isinstance(sentence, str):\n",
    "        sentence = set(sentence.split(\" \"))\n",
    "    elif isinstance(sentence, list):\n",
    "        sentence = set(sentence)\n",
    "    elif isinstance(sentence, set):\n",
    "        pass\n",
    "    else:\n",
    "        return\n",
    "    return len( gloss.intersection(sentence) )\n",
    "\n",
    "def lesk( word, sentence ):\n",
    "    \n",
    "    bestsense = None\n",
    "    maxoverlap = 0\n",
    "    for x in range(len(iwn.synsets(word))):\n",
    "        sense = iwn.synsets(word)[x];\n",
    "        overlap = overlapcontext(sense,sentence)\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = x\n",
    "    return bestsense\n",
    "\n",
    "def test(sentence,words):\n",
    "    \n",
    "    count = [0,0,0]\n",
    "    for i in words:\n",
    "        j=i[:-2];\n",
    "        if int(i[-1]) <= len(iwn.synsets(j)) :\n",
    "            print(i)\n",
    "            predicted = lesk(j,sentence);\n",
    "            original = iwn.synsets(j)[int(i[-1])-1]\n",
    "\n",
    "    \n",
    "\n",
    "            if int(i[-1])-1 == predicted :\n",
    "                count[0] = count[0] + 1;\n",
    "            elif predicted!=None :\n",
    "                count[1] = count[1] + 1;\n",
    "        else :\n",
    "            print(\"karuna-ISSUE\")\n",
    "            count[2] = count[2] + 1;   \n",
    "    \n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "file = open('/home/clabuser/Documents/nludivya/aaruni.txt',\n",
    "              encoding='utf-8').read()\n",
    "\n",
    "a=nltk.word_tokenize(file);\n",
    "\n",
    "sentence=[];\n",
    "words=[];\n",
    "count= [0,0,0]\n",
    "for i in a:\n",
    "    if(i[-1].isdigit()):\n",
    "        words.append(i);\n",
    "        i=i[:-2]\n",
    "    if(i=='ред'):\n",
    "        cc = test(sentence,words);\n",
    "        count[0] = count[0] + cc[0];\n",
    "        count[1] = count[1] + cc[1];\n",
    "        count[2] = count[2] + cc[2];\n",
    "        words[:]=[];\n",
    "        sentence[:]=[];\n",
    "    sentence.append(i);    \n",
    "\n",
    "print(count[0])\n",
    "print(count[1])\n",
    "print(count[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
